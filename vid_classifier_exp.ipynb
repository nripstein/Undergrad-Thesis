{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:04:49.805016Z",
     "start_time": "2023-10-07T19:04:46.055483Z"
    }
   },
   "id": "9c3f545904c0c2ba"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:05:42.723628Z",
     "start_time": "2023-10-07T19:05:42.719285Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_video_to_frames(video_path):\n",
    "    \"\"\"\n",
    "    Load a video and convert it into a list of frames.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path (str): Path to the video file.\n",
    "    \n",
    "    Returns:\n",
    "    - frames (list): List of frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def preprocess_frames(frames: list[np.ndarray], input_size: tuple[int, int], to_rgb: bool = True, normalize: bool = False,) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Preprocess a list of frames.\n",
    "\n",
    "    Args:\n",
    "        frames (list of numpy arrays): List of frames to be preprocessed.\n",
    "        input_size (tuple): A tuple specifying the target size for each frame (width, height).\n",
    "        to_rgb (bool): whether should convert from BGR to RGB (needed if extracting frames with OpenCV)\n",
    "        normalize (bool): whether to normalize (False for EfficientNet models, True otherwise)\n",
    "\n",
    "    Returns:\n",
    "        list of numpy arrays: List of preprocessed frames with the following transformations applied:\n",
    "            1. Resized to the specified input size.\n",
    "            2. (optional) Converted from BGR to RGB color format\n",
    "            3. (optional) normalized\n",
    "    \"\"\"\n",
    "    # Resize the frames to the specified input size\n",
    "    preprocessed_frames = [cv2.resize(frame, input_size) for frame in frames]\n",
    "    # Convert the frames from BGR to RGB color format\n",
    "    if to_rgb:\n",
    "        preprocessed_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in preprocessed_frames]\n",
    "    # normalize pixel values to between 0 and 1\n",
    "    if normalize:\n",
    "        preprocessed_frames = [frame / 255.0 for frame in preprocessed_frames]\n",
    "    return preprocessed_frames\n",
    "\n",
    "\n",
    "def predict_frames(model: tf.keras.Model, frames: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict the class for each frame.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (tf.Model): Trained TensorFlow model.\n",
    "    - frames (list): List of frames which are preprocessed from the preprocess_frames() function.\n",
    "    - input_size (tuple): Tuple indicating the input size (height, width) expected by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions (np.ndarray): Array of predictions (0s and 1s).\n",
    "    \"\"\"\n",
    "    pred_probs = model.predict(np.array(frames))\n",
    "    # binary_predictions = np.round(predictions).flatten()  # should switch to returning this this\n",
    "    return pred_probs.flatten()\n",
    "\n",
    "def main(video_path, model_path, input_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load a video and a TensorFlow model, then predict the class for each frame in the video.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path (str): Path to the video file.\n",
    "    - model_path (str): Path to the TensorFlow model file.\n",
    "    - input_size (tuple): Tuple indicating the input size (height, width) expected by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions (np.array): Array of predictions (0s and 1s).\n",
    "    \"\"\"\n",
    "    \n",
    "    frames = load_video_to_frames(video_path)\n",
    "    frames = preprocess_frames(frames, input_size, to_rgb=True, normalize=False)\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    predictions = predict_frames(model, frames)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "frames = load_video_to_frames(\"output_segments_long_att4/segment_3.mp4\")\n",
    "model = tf.keras.models.load_model(\"models/b0_2_epochs_half_data_h5.h5\")\n",
    "preprocessed_frames = preprocess_frames(frames, (224, 224))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:04:54.449782Z",
     "start_time": "2023-10-07T19:04:52.644257Z"
    }
   },
   "id": "dfb3f2acc996d0f1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 4s 345ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_frames(model, preprocessed_frames)\n",
    "# predictions = main('output_segments_long_att4/segment_3.mp4', 'models/b0_2_epochs_half_data_h5.h5')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:05:49.909099Z",
     "start_time": "2023-10-07T19:05:45.805711Z"
    }
   },
   "id": "8ad3ec487a455e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(preprocessed_frames))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-07T19:04:59.986936Z"
    }
   },
   "id": "6b8c1e78389c287a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contact_dict = {0: \"no contact\", 1: \"contact\"}\n",
    "for i, frame in enumerate(preprocessed_frames):\n",
    "    plt.imshow(frame)\n",
    "    plt.title(f\"Frame {i}, {contact_dict[np.round(predictions)[i]]}, p={predictions[i]:.2f}\")\n",
    "    plt.show()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfe6d07355e6c08a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_frames_in_grid(preprocessed_frames, predictions, contact_dict):\n",
    "    \"\"\"\n",
    "    Display frames in a 4x4 grid with text color based on prediction values.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_frames (list of numpy arrays): List of preprocessed frames to display.\n",
    "        predictions (list of floats): List of prediction values for the frames.\n",
    "        contact_dict (dict): A dictionary mapping prediction values to labels.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    num_frames = len(preprocessed_frames)\n",
    "    rows = (num_frames + 3) // 4  # Calculate the number of rows needed for the grid\n",
    "\n",
    "    for i in range(rows):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "        for j in range(4):\n",
    "            frame_idx = i * 4 + j\n",
    "\n",
    "            if frame_idx < num_frames:\n",
    "                frame = preprocessed_frames[frame_idx]\n",
    "\n",
    "                # Determine text color based on the prediction value\n",
    "                if np.round(predictions)[frame_idx] < 0.5:\n",
    "                    text_color = 'red'\n",
    "                else:\n",
    "                    text_color = 'green'\n",
    "\n",
    "                axs[j].imshow(frame)\n",
    "                axs[j].set_title(f\"Frame {frame_idx}, {contact_dict[np.round(predictions)[frame_idx]]}, p={predictions[frame_idx]:.2f}\", color=text_color)\n",
    "\n",
    "            axs[j].axis('off')  # Hide axis for empty subplots\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "display_frames_in_grid(preprocessed_frames, predictions, contact_dict)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0d90412741c54c5"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# \n",
    "# \n",
    "# def create_labeled_video1(frames, predictions, contact_dict, output_path, frame_rate=29.97):\n",
    "#     \"\"\"\n",
    "#     Create a video with colored labels above frames.\n",
    "# \n",
    "#     Args:\n",
    "#         frames (list of numpy arrays): List of frames to include in the video.\n",
    "#         predictions (list of floats): List of prediction values for the frames.\n",
    "#         contact_dict (dict): A dictionary mapping prediction values to labels.\n",
    "#         output_path (str): Path to save the output video file.\n",
    "#         frame_rate (float, optional): Frame rate of the output video. Default is 29.97 fps.\n",
    "# \n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     height, width, _ = frames[0].shape\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "#     # frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) for frame in frames]\n",
    "# \n",
    "#     for frame, prediction in zip(frames, predictions):\n",
    "#         # Determine text color based on the prediction value\n",
    "#         if np.round(prediction) < 0.5:\n",
    "#             text_color = (0, 0, 255)  # Red color for values > 0.5 (BGR format)\n",
    "#         else:\n",
    "#             text_color = (0, 255, 0)  # Green color for values <= 0.5 (BGR format)\n",
    "# \n",
    "#         # Add label text above the frame\n",
    "#         label = f\"{contact_dict[np.round(prediction)]}, p={prediction:.2f}\"\n",
    "#         cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "# \n",
    "#         out.write(frame)\n",
    "# \n",
    "#     out.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     \n",
    "#     \n",
    "# def create_labeled_video(frames, predictions, contact_dict, output_path, frame_rate=29.97):\n",
    "#     \"\"\"\n",
    "#     Create a video with colored labels above frames.\n",
    "# \n",
    "#     Args:\n",
    "#         frames (list of numpy arrays): List of frames to include in the video.\n",
    "#         predictions (list of floats): List of prediction values for the frames.\n",
    "#         contact_dict (dict): A dictionary mapping prediction values to labels.\n",
    "#         output_path (str): Path to save the output video file.\n",
    "#         frame_rate (float, optional): Frame rate of the output video. Default is 29.97 fps.\n",
    "# \n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     height, width, _ = frames[0].shape\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "#     frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) for frame in frames]\n",
    "# \n",
    "#     for frame, prediction in zip(frames, predictions):\n",
    "#         # Create a blank frame to clear previous labels\n",
    "#         blank_frame = np.zeros_like(frame)\n",
    "# \n",
    "#         # Determine text color based on the prediction value\n",
    "#         if np.round(prediction) > 0.5:\n",
    "#             text_color = (0, 0, 255)  # Red color for values > 0.5 (BGR format)\n",
    "#         else:\n",
    "#             text_color = (0, 255, 0)  # Green color for values <= 0.5 (BGR format)\n",
    "# \n",
    "#         # Add label text above the frame\n",
    "#         label = f\"Prediction: {contact_dict[np.round(prediction)]}, p={prediction:.2f}\"\n",
    "#         cv2.putText(blank_frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "# \n",
    "#         # Overlay the labeled frame on top of the blank frame\n",
    "#         result_frame = cv2.addWeighted(frame, 1, blank_frame, 1, 0)\n",
    "# \n",
    "#         out.write(result_frame)\n",
    "# \n",
    "#     out.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "def create_labeled_video_og(frames, predictions, output_path, frame_rate=29.97):\n",
    "    \"\"\"\n",
    "    Create a video with colored labels above frames.\n",
    "\n",
    "    Args:\n",
    "        frames (list of numpy arrays): List of frames to include in the video.\n",
    "        predictions (list of floats): List of prediction values for the frames.\n",
    "        contact_dict (dict): A dictionary mapping prediction values to labels.\n",
    "        output_path (str): Path to save the output video file.\n",
    "        frame_rate (float, optional): Frame rate of the output video. Default is 29.97 fps.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "    frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) for frame in frames]\n",
    "\n",
    "    for frame, prediction in zip(frames, predictions):\n",
    "        # Determine text color based on the prediction value\n",
    "        if np.round(prediction) < 0.5:\n",
    "            text_color = (0, 0, 255)  # Red color for values > 0.5 (BGR format)\n",
    "        else:\n",
    "            text_color = (0, 255, 0)  # Green color for values <= 0.5 (BGR format)\n",
    "\n",
    "        # Add label text above the frame\n",
    "        label = str(round(prediction * 100, 2)) + \"%\"\n",
    "        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "\n",
    "  \n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def create_labeled_video(frames, predictions, output_path, frame_rate=29.97):\n",
    "    \"\"\"\n",
    "    Create a video with colored labels above frames.\n",
    "\n",
    "    Args:\n",
    "        frames (list of numpy arrays): List of frames to include in the video.\n",
    "        predictions (list of floats): List of prediction values for the frames.\n",
    "        output_path (str): Path to save the output video file.\n",
    "        frame_rate (float, optional): Frame rate of the output video. Default is 29.97 fps.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) for frame in frames]\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "    for frame, prediction in zip(frames, predictions):\n",
    "        # Create a blank frame for each frame\n",
    "        blank_frame = np.zeros_like(frame)\n",
    "\n",
    "        # Determine text color based on the prediction value\n",
    "        if np.round(prediction) < 0.5:\n",
    "            text_color = (0, 0, 255)  # Red color for values > 0.5 (BGR format)\n",
    "        else:\n",
    "            text_color = (0, 255, 0)  # Green color for values <= 0.5 (BGR format)\n",
    "\n",
    "        # Add label text above the frame\n",
    "        label = str(int(prediction * 100)) + \"%\"\n",
    "        cv2.putText(blank_frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "\n",
    "        # Overlay the labeled frame on top of the original frame\n",
    "        result_frame = cv2.addWeighted(frame, 1, blank_frame, 1, 0)\n",
    "\n",
    "        out.write(result_frame)\n",
    "\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "create_labeled_video(preprocessed_frames, predictions, \"tmp_label_vid2.mp4\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T19:30:54.875098Z",
     "start_time": "2023-10-07T19:30:54.732743Z"
    }
   },
   "id": "3ae8a67dd9d27434"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
